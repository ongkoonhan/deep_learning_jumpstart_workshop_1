{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "from torchsummary import summary\n",
    "\n",
    "from pyeer.eer_info import get_eer_stats\n",
    "from pyeer.report import generate_eer_report, export_error_rates\n",
    "from pyeer.plot import plot_eer_stats\n",
    "\n",
    "from config import models_folder, output_data_folder, EER_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = os.path.join(EER_folder, \"mobilenetv2_vs_densenet121\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model input config\n",
    "parent_dir = os.path.join(models_folder, \"verification_classifier\", \"good_models\")\n",
    "model_folders = [\n",
    "    \"2020-03-20_12-20-57_mobilenet\",\n",
    "    \"2020-04-06_16-34-21_densenet\",\n",
    "    \"2020-04-10_13-13-49_var_red_mobilenet\",\n",
    "    \"2020-04-10_15-21-25_var_red_densenet\",\n",
    "]\n",
    "model_folders = [os.path.join(parent_dir, folder) for folder in model_folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EER config\n",
    "EER_model_names = [\n",
    "    \"MobileNetV2\", \"DenseNet121\",\n",
    "    \"MobileNetV2_VR\", \"DenseNet121_VR\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output folder exists check\n",
    "if os.path.exists(output_folder): raise FileExistsError(\"{} already exists\".format(output_folder))\n",
    "else: os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_scores(model_folder, validation_data):\n",
    "    # Verification Binary classifier\n",
    "    model = torch.jit.load(os.path.join(model_folder, \"mobile_model.pt\"))\n",
    "    model.to(device)\n",
    "    \n",
    "    ### scores\n",
    "    class_probs_all = []\n",
    "    outputs_all = []\n",
    "    labels_all = []\n",
    "    geniune_scores_all = []\n",
    "    impostor_scores_all = []\n",
    "\n",
    "    for data in validation_data:\n",
    "        # prep inputs\n",
    "        input_imgs, labels = data\n",
    "        inputs = [img.to(device) for img in input_imgs]\n",
    "        # predict\n",
    "        with torch.no_grad():\n",
    "            model.eval()   # eval mode\n",
    "            outputs = model(inputs)\n",
    "            class_probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "\n",
    "        # save model outputs\n",
    "        # Convert to numpy\n",
    "        class_probs = class_probs.cpu().detach().numpy()\n",
    "        outputs = outputs.cpu().detach().numpy()\n",
    "        labels = labels.cpu().detach().numpy()\n",
    "        # append arrays\n",
    "        class_probs_all.append(class_probs)\n",
    "        outputs_all.append(outputs)\n",
    "        labels_all.append(labels)\n",
    "        # genuine/impostor scores\n",
    "        genuine_idx = np.argwhere(labels == 1)\n",
    "        imposter_idx = np.argwhere(labels == 0)\n",
    "        # append values\n",
    "        geniune_scores_all.extend(class_probs[genuine_idx, 1].flatten().tolist())\n",
    "        impostor_scores_all.extend(class_probs[imposter_idx, 1].flatten().tolist())\n",
    "    \n",
    "    # Save scores\n",
    "    scores_save_config = [\n",
    "        (class_probs_all, \"class_probs_all.pickle\"),\n",
    "        (outputs_all, \"outputs_all.pickle\"),\n",
    "        (labels_all, \"labels_all.pickle\"),\n",
    "        (geniune_scores_all, \"geniune_scores_all.pickle\"),\n",
    "        (impostor_scores_all, \"impostor_scores_all.pickle\"),\n",
    "    ]\n",
    "    # Scores folder\n",
    "    scores_folder = os.path.join(output_folder, os.path.basename(model_folder))\n",
    "    if not os.path.exists(scores_folder): os.makedirs(scores_folder)\n",
    "    for list_, filename in scores_save_config:\n",
    "        file = os.path.join(scores_folder, filename)\n",
    "        with open(file, 'wb') as handle:\n",
    "            pickle.dump(list_, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        # Print count stats\n",
    "        print(\"{}, Count: {}\".format(filename, len(list_)))\n",
    "        \n",
    "    # For EER stats\n",
    "    return geniune_scores_all, impostor_scores_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation data\n",
    "validation_set_file = os.path.join(output_data_folder, \"validation_sets\", \"verification_validation_set.pickle\")\n",
    "with open(validation_set_file, 'rb') as f:\n",
    "    validation_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_probs_all.pickle, Count: 12\n",
      "outputs_all.pickle, Count: 12\n",
      "labels_all.pickle, Count: 12\n",
      "geniune_scores_all.pickle, Count: 1920\n",
      "impostor_scores_all.pickle, Count: 1920\n",
      "class_probs_all.pickle, Count: 12\n",
      "outputs_all.pickle, Count: 12\n",
      "labels_all.pickle, Count: 12\n",
      "geniune_scores_all.pickle, Count: 1920\n",
      "impostor_scores_all.pickle, Count: 1920\n",
      "class_probs_all.pickle, Count: 12\n",
      "outputs_all.pickle, Count: 12\n",
      "labels_all.pickle, Count: 12\n",
      "geniune_scores_all.pickle, Count: 1920\n",
      "impostor_scores_all.pickle, Count: 1920\n",
      "class_probs_all.pickle, Count: 12\n",
      "outputs_all.pickle, Count: 12\n",
      "labels_all.pickle, Count: 12\n",
      "geniune_scores_all.pickle, Count: 1920\n",
      "impostor_scores_all.pickle, Count: 1920\n"
     ]
    }
   ],
   "source": [
    "### Get scores\n",
    "scores = []\n",
    "for model_folder in model_folders:\n",
    "    score = generate_model_scores(model_folder, validation_data)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EER Stats\n",
    "eer_stats_list = []\n",
    "# Calculating stats for classifiers\n",
    "for geniune_scores_all, impostor_scores_all in scores:\n",
    "    eer_stats = get_eer_stats(geniune_scores_all, impostor_scores_all)\n",
    "    eer_stats_list.append(eer_stats)\n",
    "# Generating report\n",
    "generate_eer_report([*eer_stats_list], [*EER_model_names], os.path.join(output_folder, 'pyeer_report.csv'))\n",
    "generate_eer_report([*eer_stats_list], [*EER_model_names], os.path.join(output_folder, 'pyeer_report.html'))\n",
    "# Exporting error rates (Exporting FMR and FNMR to a CSV file)\n",
    "# This is the DET curve, the ROC curve is a plot of FMR against 1 - FNMR\n",
    "for eer_stats, model_name in zip(eer_stats_list, EER_model_names):\n",
    "    out_file = os.path.join(output_folder, 'DET_{}.csv'.format(model_name))\n",
    "    export_error_rates(eer_stats.fmr, eer_stats.fnmr, out_file)\n",
    "# Plotting\n",
    "plot_eer_stats([*eer_stats_list], [*EER_model_names], save_path=output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
